# Training & Evaluation Modules

Complete training and evaluation infrastructure for the hybrid neural-symbolic model.

---

## üìÅ Files (88 KB total)

### Training (25 KB)
| File | Size | Description |
|------|------|-------------|
| `train.py` | 13 KB | Multi-stage training pipeline |
| `hyperparameter_optimization.py` | 12 KB | Bayesian optimization with Optuna |

### Evaluation (63 KB)
| File | Size | Description |
|------|------|-------------|
| `metrics.py` | 18 KB | All evaluation metrics |
| `evaluate.py` | 13 KB | Evaluation pipeline |
| `error_analysis.py` | 17 KB | Error categorization |
| `baseline_comparison.py` | 15 KB | Compare with BART, GPT-4 |

---

## üéØ Training Module

### 1. Multi-Stage Training (`train.py`)

**Purpose:** Implements Algorithm 6 (Multi-Stage Training) from the paper

**Training Stages:**
1. **Stage 1: Warm-up (3 epochs)**
   - Train decoder only
   - Learning rate: 1e-4
   - Focus on basic summarization

2. **Stage 2: Knowledge Integration (2 epochs)**
   - Train encoders + KG components
   - Learning rate: 5e-5
   - Add KG alignment loss

3. **Stage 3: End-to-End Fine-tuning (5 epochs)**
   - Train all parameters
   - Learning rate: 2e-5
   - Complete loss function (Equation 10)

**Loss Function (Equation 10):**
```
L_total = Œ±*L_summary + Œ≤*L_KG + Œ≥*L_explanation
```
Default weights: Œ±=0.7, Œ≤=0.2, Œ≥=0.1

**Usage:**
```python
from training.train import MultiStageTrainer

# Initialize
trainer = MultiStageTrainer(
    model=model,
    train_dataloader=train_loader,
    val_dataloader=val_loader,
    config={
        'stage1_lr': 1e-4,
        'stage2_lr': 5e-5,
        'stage3_lr': 2e-5,
        'alpha': 0.7,
        'beta': 0.2,
        'gamma': 0.1
    },
    device='cuda'
)

# Train
trainer.train(output_dir='checkpoints/')
```

**Outputs:**
- `best_model_stage{1,2,3}.pt` - Best models per stage
- `stage{1,2,3}_final.pt` - Final checkpoints
- `training_history.json` - Training curves

**Training Time:**
- Stage 1: ~8 hours (3 epochs)
- Stage 2: ~6 hours (2 epochs)
- Stage 3: ~12 hours (5 epochs)
- **Total: ~26-30 hours** (with single GPU)

---

### 2. Hyperparameter Optimization (`hyperparameter_optimization.py`)

**Purpose:** Bayesian optimization for hyperparameter tuning

**Optimized Parameters:**

**Learning Rates:**
- `stage1_lr`: [1e-5, 5e-4]
- `stage2_lr`: [1e-6, 1e-4]
- `stage3_lr`: [1e-6, 5e-5]

**Loss Weights:**
- `alpha`: [0.5, 0.9]
- `beta`: [0.1, 0.3]
- `gamma`: derived (1 - Œ± - Œ≤)

**Architecture:**
- `hidden_dim`: [512, 768, 1024]
- `num_attention_heads`: [4, 8, 12]
- `num_reasoning_hops`: [2, 3, 4]
- `num_gat_layers`: [2, 3, 4]
- `gat_heads`: [4, 8]

**Dropout:**
- `dropout`: [0.1, 0.3]
- `attention_dropout`: [0.1, 0.2]

**Training:**
- `batch_size`: [8, 16, 32]
- `warmup_ratio`: [0.05, 0.15]

**Usage:**
```python
from training.hyperparameter_optimization import HyperparameterOptimizer

# Initialize
optimizer = HyperparameterOptimizer(
    model_class=HybridModel,
    train_fn=train_function,
    eval_fn=eval_function,
    n_trials=50,
    study_name='financial_summarization'
)

# Run optimization
best_params = optimizer.optimize(direction='minimize')

# Outputs: hp_optimization_results/best_params.json
```

**Optimization Methods:**
1. **Bayesian Optimization** (Recommended)
   - Sampler: TPE (Tree-structured Parzen Estimator)
   - Pruner: Median (early stopping)
   - Trials: 50
   - Time: ~8-12 hours

2. **Grid Search** (Alternative)
   - Exhaustive search
   - Faster but less efficient
   - Good for quick experiments

**Expected Output:**
```json
{
  "stage1_lr": 8.3e-05,
  "stage2_lr": 3.2e-05,
  "stage3_lr": 1.5e-05,
  "alpha": 0.72,
  "beta": 0.18,
  "gamma": 0.10,
  "hidden_dim": 768,
  "num_attention_heads": 8,
  "num_reasoning_hops": 3
}
```

---

## üìä Evaluation Module

### 3. Evaluation Metrics (`metrics.py`)

**Purpose:** Comprehensive metric computation

**Summarization Metrics:**

#### ROUGE Scores
```python
from evaluation.metrics import ROUGEMetrics

rouge = ROUGEMetrics()
scores = rouge.compute(predictions, references)
# Returns: {'rouge1': 0.45, 'rouge2': 0.32, 'rougeL': 0.487}
```

#### BERTScore
```python
from evaluation.metrics import BERTScoreMetrics

bertscore = BERTScoreMetrics(model_type="microsoft/deberta-xlarge-mnli")
scores = bertscore.compute(predictions, references)
# Returns: {'bertscore': 0.686, 'precision': 0.68, 'recall': 0.69}
```

#### Factual Consistency
```python
from evaluation.metrics import FactualConsistencyMetrics

factual = FactualConsistencyMetrics()
scores = factual.compute(predictions, sources)
# Returns: {'factual_consistency': 87.6}
```

**Novel Explainability Metrics:**

#### SSI (Stakeholder Satisfaction Index) - Equation 5
```python
from evaluation.metrics import SSIMetrics

ssi = SSIMetrics()
scores = ssi.compute(
    explanations={'analyst': [...], 'compliance': [...]},
    stakeholder_ratings={'analyst': [0.8, 0.9], 'compliance': [0.7, 0.85]}
)
# Returns: {'ssi': 0.74, 'ssi_std': 0.06}
```

**Weights:** analyst=0.30, compliance=0.25, executive=0.25, investor=0.20

#### CPS (Causal Preservation Score) - Equations 7-8
```python
from evaluation.metrics import CPSMetrics

cps = CPSMetrics()
scores = cps.compute(
    predicted_chains=[...],
    reference_chains=[...],
    importance_weights=[0.9, 0.7, 0.5]
)
# Returns: {'cps': 0.51, 'cps_weighted': 0.53}
```

#### TCC (Temporal Consistency Coefficient) - Equation 9
```python
from evaluation.metrics import TCCMetrics

tcc = TCCMetrics()
scores = tcc.compute(
    explanation_sequence=["Explanation at t1", "Explanation at t2", ...]
)
# Returns: {'tcc': 0.54, 'tcc_std': 0.07}
```

Formula: TCC = (1/N) Œ£ cos_sim(E_t, E_{t-1})

#### RCS (Regulatory Compliance Score)
```python
from evaluation.metrics import RCSMetrics

rcs = RCSMetrics()
scores = rcs.compute(
    explanations=[...],
    requirements=['GDPR Article 13', 'EU AI Act Article 52']
)
# Returns: {'rcs': 92.5, 'rcs_std': 5.2}
```

**Combined Evaluator:**
```python
from evaluation.metrics import ComprehensiveEvaluator

evaluator = ComprehensiveEvaluator()
all_metrics = evaluator.evaluate_all(
    predictions=predictions,
    references=references,
    sources=sources,
    explanations=explanations,
    stakeholder_ratings=ratings,
    predicted_chains=chains,
    reference_chains=ref_chains
)
```

---

### 4. Evaluation Pipeline (`evaluate.py`)

**Purpose:** Complete evaluation workflow

**Usage:**
```python
from evaluation.evaluate import EvaluationPipeline

pipeline = EvaluationPipeline(
    model=trained_model,
    tokenizer=tokenizer,
    device='cuda'
)

results = pipeline.evaluate(
    test_dataloader=test_loader,
    output_dir='evaluation_results/',
    save_predictions=True
)
```

**Evaluation Steps:**
1. Generate predictions
2. Compute all metrics
3. Analyze errors
4. Compute computational costs
5. Save results

**Outputs:**
- `metrics.json` - All metric scores
- `evaluation_results.json` - Complete results
- `predictions.jsonl` - Model predictions
- Training curves and statistics

**Output Example:**
```json
{
  "metrics": {
    "rougeL": 0.487,
    "bertscore": 0.686,
    "factual_consistency": 87.6,
    "ssi": 0.74,
    "cps": 0.51,
    "tcc": 0.54
  },
  "costs": {
    "avg_inference_time": 2.9,
    "memory_allocated_gb": 9.8,
    "num_parameters": 416000000
  }
}
```

---

### 5. Error Analysis (`error_analysis.py`)

**Purpose:** Detailed error categorization per Section V.D

**Error Categories (from paper):**
1. **Entity Misidentification** (21.8%)
   - Hallucinated entities
   - Missing entities
   - Wrong entity types

2. **Causal Misattribution** (15.2%)
   - Incorrect causation
   - Missing causal links
   - Reversed cause-effect

3. **Temporal Inconsistency** (12.5%)
   - Wrong timeframes
   - Incorrect ordering
   - Missing temporal markers

4. **Factual Errors** (7.0%)
   - Hallucinated numbers
   - Incorrect facts
   - Contradictions

5. **Other** (43.5%)
   - Miscellaneous errors

**Severity Levels:**
- **Low (0-33%):** Minor issues
- **Medium (34-66%):** Moderate issues
- **High (67-100%):** Critical issues

**Usage:**
```python
from evaluation.error_analysis import ErrorAnalyzer

analyzer = ErrorAnalyzer()
error_report = analyzer.analyze(
    predictions=predictions,
    references=references,
    sources=sources
)

# Access results
print(error_report['statistics'])
print(error_report['errors_by_category'])
print(error_report['errors_by_severity'])
```

**Output:**
```
ERROR ANALYSIS SUMMARY
======================================================================
Total Samples: 500
Total Errors: 283
Error Rate: 56.6%

Error Distribution by Category:
  entity_misidentification: 62 (21.9%)
  causal_misattribution: 43 (15.2%)
  temporal_inconsistency: 35 (12.4%)
  factual_error: 20 (7.1%)
  other: 123 (43.5%)

Severity Distribution:
  low: 98 (34.6%)
  medium: 123 (43.5%)
  high: 62 (21.9%)
```

---

### 6. Baseline Comparison (`baseline_comparison.py`)

**Purpose:** Compare against baseline models

**Baselines:**
1. **Baseline BART** (facebook/bart-large)
   - ROUGE-L: 0.421
   - BERTScore: 0.612
   - Factual: 83.7%

2. **SOTA Hybrid**
   - ROUGE-L: 0.465
   - BERTScore: 0.658
   - Factual: 86.2%

3. **Our System**
   - ROUGE-L: 0.487 (**+15.7%**)
   - BERTScore: 0.686 (**+12.1%**)
   - Factual: 87.6% (**+4.7pp**)

**Usage:**
```python
from evaluation.baseline_comparison import BaselineComparator

comparator = BaselineComparator()

comparison = comparator.compare(
    our_results=evaluation_results,
    save_dir='comparison_results/'
)

# Outputs:
# - baseline_comparison_summarization.png
# - baseline_comparison_explainability.png
# - baseline_comparison.json
# - baseline_comparison.md
```

**Comparison Table (Table II):**

| Metric | Baseline | SOTA | Ours | Improvement |
|--------|----------|------|------|-------------|
| ROUGE-L | 0.421 | 0.465 | 0.487 | +15.7% |
| BERTScore | 0.612 | 0.658 | 0.686 | +12.1% |
| Factual | 83.7% | 86.2% | 87.6% | +4.7pp |

**Explainability Comparison (Table III):**

| Metric | Baseline | SOTA | Ours | Improvement |
|--------|----------|------|------|-------------|
| SSI | 0.61¬±0.07 | 0.67¬±0.07 | 0.74¬±0.06 | +21.3% |
| CPS | 0.34¬±0.08 | 0.42¬±0.08 | 0.51¬±0.07 | +50.0% |
| TCC | 0.38¬±0.06 | 0.45¬±0.07 | 0.54¬±0.07 | +42.1% |
| Consistency | 0.43¬±0.07 | 0.51¬±0.07 | 0.59¬±0.06 | +37.2% |

---

## üöÄ Quick Start

### Training
```bash
# Train with default settings
python train.py \
    --config configs/training_config.yaml \
    --data_path data/financial_corpus \
    --output_dir checkpoints/

# Multi-stage training
python train.py \
    --multi_stage \
    --stage1_epochs 3 \
    --stage2_epochs 2 \
    --stage3_epochs 5
```

### Hyperparameter Optimization
```bash
# Run Bayesian optimization
python hyperparameter_optimization.py \
    --n_trials 50 \
    --study_name financial_summarization \
    --output_dir hp_results/
```

### Evaluation
```bash
# Evaluate checkpoint
python evaluate.py \
    --checkpoint checkpoints/best_model.pt \
    --test_data data/test.json \
    --output_dir evaluation_results/

# With baseline comparison
python evaluate.py \
    --checkpoint checkpoints/best_model.pt \
    --test_data data/test.json \
    --compare_baselines \
    --output_dir evaluation_results/
```

---

## üìà Performance Benchmarks

### Training Performance
- **GPU:** NVIDIA A100 (40GB)
- **Batch Size:** 16
- **Total Time:** 26-30 hours (10 epochs)
- **Peak Memory:** 24 GB
- **Throughput:** ~450 samples/hour

### Inference Performance
- **Time per Document:** 2.9s
- **Batch Inference:** 8 docs/batch @ 3.5s
- **Memory:** 9.8 GB
- **Throughput:** ~1,235 docs/hour

---

## üîß Configuration

### Training Config (`training_config.yaml`)
```yaml
# Learning rates
stage1_lr: 1.0e-4
stage2_lr: 5.0e-5
stage3_lr: 2.0e-5

# Loss weights
alpha: 0.7  # Summary loss
beta: 0.2   # KG alignment
gamma: 0.1  # Explanation

# Training
batch_size: 16
num_epochs: [3, 2, 5]
max_grad_norm: 1.0
warmup_ratio: 0.1

# Optimization
optimizer: AdamW
weight_decay: 0.01
scheduler: linear_with_warmup
```

---

## üìù Example Workflow

```python
# 1. Train model
from training.train import MultiStageTrainer

trainer = MultiStageTrainer(model, train_loader, val_loader, config)
trainer.train('checkpoints/')

# 2. Evaluate
from evaluation.evaluate import EvaluationPipeline

pipeline = EvaluationPipeline(model, tokenizer)
results = pipeline.evaluate(test_loader, 'results/')

# 3. Analyze errors
from evaluation.error_analysis import ErrorAnalyzer

analyzer = ErrorAnalyzer()
error_report = analyzer.analyze(predictions, references, sources)

# 4. Compare with baselines
from evaluation.baseline_comparison import BaselineComparator

comparator = BaselineComparator()
comparison = comparator.compare(results, 'comparison/')
```

---

## ‚úÖ Testing

```bash
# Test training
python training/train.py

# Test evaluation
python evaluation/evaluate.py

# Test metrics
python evaluation/metrics.py

# Test error analysis
python evaluation/error_analysis.py

# Test comparison
python evaluation/baseline_comparison.py
```

---

## üöÄ Ready for Training & Evaluation!

All modules are production-ready with comprehensive documentation!
